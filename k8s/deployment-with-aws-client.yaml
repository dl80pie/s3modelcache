kind: Deployment
apiVersion: apps/v1
metadata:
  name: vllm-inferenceall
  labels:
    app: vllm-openai
spec:
  replicas: 0
  selector:
    matchLabels:
      app: vllm-openai
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: vllm-openai
    spec:
      nodeSelector:
        nvidia.com/gpu.count: '2'
      restartPolicy: Always
      initContainers:
        - resources: {}
          terminationMessagePath: /dev/termination-log
          name: model-downloader
          command:
            - /bin/sh
            - '-c'
          env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: access_key
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: secret_key
            - name: AWS_ENDPOINT_URL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: endpoint
          imagePullPolicy: Always
          volumeMounts:
            - name: model-cache
              mountPath: /data
          terminationMessagePolicy: File
          image: 'amazon/aws-cli:latest'
          args:
            - |
              echo "Starte Download..."; aws s3 sync s3://models/models/v1/microsoft_Phi-3-mini-4k-instruct /data/models/openai_gpt-oss-120b --endpoint-url=$AWS_ENDPOINT_URL --no-verify-ssl;
      schedulerName: default-scheduler
      terminationGracePeriodSeconds: 30
      securityContext: {}
      containers:
        - name: vllm
          image: 'registry.redhat.io/rhaiis/vllm-cuda-rhel9:latest'
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              nvidia.com/gpu: '2'
            requests:
              cpu: '8'
              memory: 32Gi
              nvidia.com/gpu: '2'
          command:
            - /bin/sh
            - -c
          args:
            - |
              python3 -m vllm.entrypoints.openai.api_server ${VLLM_ARGS}
          env:
            - name: VLLM_ARGS
              valueFrom:
                configMapKeyRef:
                  name: vllm-config
                  key: vllm-args
            - name: HOME
              value: /data
            - name: HF_HOME
              value: /data/huggingface
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: access_key
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: secret_key
            - name: AWS_ENDPOINT_URL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: endpoint
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            timeoutSeconds: 1
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 120
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          securityContext: {}
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: model-cache
              mountPath: /data
          terminationMessagePolicy: File
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 4Gi
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
      dnsPolicy: ClusterFirst