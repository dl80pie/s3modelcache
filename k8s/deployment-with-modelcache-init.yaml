apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-inference
  namespace: default
  labels:
    app: my-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-inference
  template:
    metadata:
      labels:
        app: my-inference
    spec:
      initContainers:
        - name: model-cache-init
          image: harbor.pietsch.uk/library/s3modelcache:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: MODEL_ID
              value: "microsoft/Phi-3-mini-4k-instruct"
            - name: MODEL_CACHE_DIR
              value: "/cache/models"
            - name: S3_ENDPOINT
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: endpoint
            - name: S3_BUCKET
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: bucket
            - name: S3_REGION
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: region
            - name: S3_PREFIX
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: prefix
            - name: S3_VERIFY_SSL
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: verify_ssl
            - name: S3_STORE_AS_ARCHIVE
              value: "false"
            - name: S3_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: access_key
            - name: S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-credentials
                  key: secret_key
            - name: HUGGINGFACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          volumeMounts:
            - name: model-cache
              mountPath: /cache
      containers:
        - name: vllm
          image: 'registry.redhat.io/rhaiis/vllm-cuda-rhel9:latest'
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              nvidia.com/gpu: '2'
            requests:
              cpu: '8'
              memory: 32Gi
              nvidia.com/gpu: '2'
          command:
            - /bin/sh
            - -c
          args:
            - |
              python3 -m vllm.entrypoints.openai.api_server ${VLLM_ARGS}
          env:
            - name: VLLM_ARGS
              valueFrom:
                configMapKeyRef:
                  name: vllm-config
                  key: vllm-args
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            timeoutSeconds: 1
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 120
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          volumeMounts:
            - name: model-cache
              mountPath: /cache
          ports:
            - containerPort: 8000
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
